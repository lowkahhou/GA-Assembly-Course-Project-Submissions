{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapping\n",
    "1) Scrape all the main urls from Straits Times\n",
    "\n",
    "2) Scrape content from each main url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #To scrape for Singapore News\n",
    "\n",
    "# search_field = ['courts-crime', 'education', 'housing', 'transport', 'health', 'manpower', 'environment']\n",
    "# df = pd.DataFrame()\n",
    "\n",
    "# def scrap(feature, source):\n",
    "#             try: \n",
    "#                 feature.append(source)    \n",
    "#             except:\n",
    "#                 feature.append(np.nan)\n",
    "\n",
    "# for search in search_field:\n",
    "#     base_url = 'https://www.straitstimes.com/singapore/' + search + '?page='\n",
    "#     page_no = 0\n",
    "#     loop = True\n",
    "#     print('scraping', search)\n",
    "    \n",
    "#     while loop == True:\n",
    "#         url = base_url + str(page_no)\n",
    "#         browser = webdriver.Firefox()\n",
    "#         browser.get(url)\n",
    "#         # sleep(3)\n",
    "#         source = browser.page_source\n",
    "#         browser.quit()\n",
    "#         soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "#         links = []\n",
    "#         titles = []\n",
    "#         dates = []\n",
    "\n",
    "#         for link in soup.find_all('a', class_='block-link'):\n",
    "#             scrap(links, link['href']) \n",
    "#             scrap(titles, link['title'])\n",
    "\n",
    "#         for link in soup.find_all('div', class_='node-postdate'):\n",
    "#             scrap(dates, link.text)\n",
    "\n",
    "#         links = ['https://www.straitstimes.com' + i for i in links if i != np.nan]\n",
    "\n",
    "#         assert(len(links) == len(titles))\n",
    "#         assert(len(titles) == len(dates))\n",
    "\n",
    "#         temp_df = pd.DataFrame([links,titles,dates]).T\n",
    "#         temp_df.columns = ['links', 'titles', 'dates']\n",
    "#         temp_df['search_field'] = search\n",
    "#         temp_df['page_no'] = page_no\n",
    "#         df = pd.concat([df, temp_df])\n",
    "#         df.to_csv('news_links_sgcat.csv')\n",
    "        \n",
    "#         if soup.find_all('li', class_='pager-next')[0].text == '››':\n",
    "#             page_no += 1\n",
    "#         else:\n",
    "#             loop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To scrape for Business News (may have some Singapore News)\n",
    "\n",
    "# search_field = ['companies-markets', 'economy', 'banking', 'property', 'invest']\n",
    "# df = pd.DataFrame()\n",
    "\n",
    "# def scrap(feature, source):\n",
    "#     try: \n",
    "#         feature.append(source)    \n",
    "#     except:\n",
    "#         feature.append(np.nan)\n",
    "\n",
    "# for search in search_field:\n",
    "#     base_url = 'https://www.straitstimes.com/business/' + search + '?page='\n",
    "#     page_no = 0\n",
    "#     loop = True\n",
    "#     print('scraping', search)\n",
    "    \n",
    "#     while loop == True:\n",
    "#         url = base_url + str(page_no)\n",
    "#         browser = webdriver.Firefox()\n",
    "#         browser.get(url)\n",
    "#         # sleep(3)\n",
    "#         source = browser.page_source\n",
    "#         browser.quit()\n",
    "#         soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "#         links = []\n",
    "#         titles = []\n",
    "#         dates = []\n",
    "\n",
    "#         for link in soup.find_all('a', class_='block-link'):\n",
    "#             scrap(links, link['href']) \n",
    "#             scrap(titles, link['title'])\n",
    "\n",
    "#         for link in soup.find_all('div', class_='node-postdate'):\n",
    "#             scrap(dates, link.text)\n",
    "\n",
    "#         links = ['https://www.straitstimes.com' + i for i in links if i != np.nan]\n",
    "\n",
    "#         assert(len(links) == len(titles))\n",
    "#         assert(len(titles) == len(dates))\n",
    "\n",
    "#         temp_df = pd.DataFrame([links,titles,dates]).T\n",
    "#         temp_df.columns = ['links', 'titles', 'dates']\n",
    "#         temp_df['search_field'] = search\n",
    "#         temp_df['page_no'] = page_no\n",
    "#         df = pd.concat([df, temp_df])\n",
    "#         df.to_csv('news_links_business.csv')\n",
    "        \n",
    "#         if soup.find_all('li', class_='pager-next')[0].text == '››':\n",
    "#             page_no += 1\n",
    "#         else:\n",
    "#             loop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To scrape for General News (may have some Singapore News)\n",
    "\n",
    "# # search_field = ['politics', 'asia', 'world', 'business', 'tech']\n",
    "# search_field = 'politics'\n",
    "# df = pd.DataFrame()\n",
    "\n",
    "# def scrap(feature, source):\n",
    "#             try: \n",
    "#                 feature.append(source)    \n",
    "#             except:\n",
    "#                 feature.append(np.nan)\n",
    "\n",
    "# for search in search_field:\n",
    "#     base_url = 'https://www.straitstimes.com/' + search + '/latest?page='\n",
    "#     page_no = 0\n",
    "#     loop = True\n",
    "#     print('scraping', search)\n",
    "    \n",
    "#     while loop == True:\n",
    "#         url = base_url + str(page_no)\n",
    "#         browser = webdriver.Firefox()\n",
    "#         browser.get(url)\n",
    "#         # sleep(3)\n",
    "#         source = browser.page_source\n",
    "#         browser.quit()\n",
    "#         soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "#         links = []\n",
    "#         titles = []\n",
    "#         dates = []\n",
    "\n",
    "#         for link in soup.find_all('a', class_='block-link'):\n",
    "#             scrap(links, link['href']) \n",
    "#             scrap(titles, link['title'])\n",
    "\n",
    "#         for link in soup.find_all('div', class_='node-postdate'):\n",
    "#             scrap(dates, link.text)\n",
    "\n",
    "#         links = ['https://www.straitstimes.com' + i for i in links if i != np.nan]\n",
    "\n",
    "#         assert(len(links) == len(titles))\n",
    "#         assert(len(titles) == len(dates))\n",
    "\n",
    "#         temp_df = pd.DataFrame([links,titles,dates]).T\n",
    "#         temp_df.columns = ['links', 'titles', 'dates']\n",
    "#         temp_df['search_field'] = search\n",
    "#         temp_df['page_no'] = page_no\n",
    "#         df = pd.concat([df, temp_df])\n",
    "#         df.to_csv('news_links.csv')\n",
    "        \n",
    "#         if soup.find_all('li', class_='pager-next')[0].text == '››':\n",
    "#             page_no += 1\n",
    "#         else:\n",
    "#             loop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To scrape for Singapore Latest News\n",
    "\n",
    "# df = pd.DataFrame()\n",
    "\n",
    "# def scrap(feature, source):\n",
    "#             try: \n",
    "#                 feature.append(source)    \n",
    "#             except:\n",
    "#                 feature.append(np.nan)\n",
    "\n",
    "# base_url = 'https://www.straitstimes.com/singapore/latest?page='\n",
    "# page_no = 0\n",
    "# loop = True\n",
    "    \n",
    "# while loop == True:\n",
    "#     print('scraping', page_no)\n",
    "#     url = base_url + str(page_no)\n",
    "#     browser = webdriver.Firefox()\n",
    "#     browser.get(url)\n",
    "#     # sleep(3)\n",
    "#     source = browser.page_source\n",
    "#     browser.quit()\n",
    "#     soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "#     links = []\n",
    "#     titles = []\n",
    "#     dates = []\n",
    "\n",
    "#     for link in soup.find_all('a', class_='block-link'):\n",
    "#         scrap(links, link['href']) \n",
    "#         scrap(titles, link['title'])\n",
    "\n",
    "#     for link in soup.find_all('div', class_='node-postdate'):\n",
    "#         scrap(dates, link.text)\n",
    "\n",
    "#     links = ['https://www.straitstimes.com' + i for i in links if i != np.nan]\n",
    "\n",
    "#     assert(len(links) == len(titles))\n",
    "#     assert(len(titles) == len(dates))\n",
    "\n",
    "#     temp_df = pd.DataFrame([links,titles,dates]).T\n",
    "#     temp_df.columns = ['links', 'titles', 'dates']\n",
    "#     temp_df['search_field'] = 'others'\n",
    "#     temp_df['page_no'] = page_no\n",
    "#     df = pd.concat([df, temp_df])\n",
    "#     df.to_csv('news_links_sgothers.csv')\n",
    "        \n",
    "#     if soup.find_all('li', class_='pager-next')[0].text == '››':\n",
    "#         page_no += 1\n",
    "#     else:\n",
    "#         loop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To scrape for Articles from the list of links\n",
    "\n",
    "# main_links = pd.read_csv('main_data.csv')\n",
    "# df = pd.DataFrame()\n",
    "\n",
    "# for index, link in enumerate(main_links['url'][3430:]):\n",
    "#     is_blank = 0\n",
    "    \n",
    "#     while is_blank < 8:\n",
    "#         titles = []\n",
    "#         authors = []\n",
    "#         articles = []\n",
    "#         print('scraping', index)\n",
    "#         url = link\n",
    "#         source = requests.get(url).text\n",
    "#         soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "#         try:\n",
    "#             titles.append(soup.find('h1', class_='headline node-title').text)\n",
    "#         except:\n",
    "#             titles.append(np.nan)\n",
    "\n",
    "#         try:\n",
    "#             authors.append(soup.find('div', class_='author-field author-name').text)\n",
    "#         except:\n",
    "#             authors.append(np.nan)\n",
    "\n",
    "#         article = str()\n",
    "#         for each_section in soup.find_all('div', class_='odd field-item'):\n",
    "#             for each_para in each_section.find_all('p'):\n",
    "#                 article += each_para.text + ' '\n",
    "#         articles.append(article)\n",
    "\n",
    "#         assert(len(titles) == len(authors))\n",
    "#         assert(len(authors) == len(articles))\n",
    "        \n",
    "#         if article != '':\n",
    "#             temp_df = pd.DataFrame([titles, authors, articles]).T\n",
    "#             temp_df.columns = ['titles', 'authors', 'articles']\n",
    "#             temp_df['url'] = url\n",
    "\n",
    "#             date = main_links.loc[index, 'dates']\n",
    "#             temp_df['dates'] = date\n",
    "#             search_field = main_links.loc[index, 'category']\n",
    "#             temp_df['category'] = search_field\n",
    "\n",
    "#             df = pd.concat([df, temp_df])\n",
    "#             df.to_csv('news_articles.csv')\n",
    "#             is_blank = 99\n",
    "            \n",
    "#         elif (is_blank == 7):\n",
    "#             print('this row has null value for article')\n",
    "#             na_df = pd.DataFrame([np.nan]*6).T\n",
    "#             df = pd.concat([df, na_df])\n",
    "#             df.to_csv('news_articles.csv')\n",
    "#             is_blank = 99\n",
    "            \n",
    "#         else:\n",
    "#             is_blank += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://www.straitstimes.com/politics/latest?page=1\"\n",
    "\n",
    "# # source = requests.get(url).text\n",
    "# # soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "# browser = webdriver.Firefox()\n",
    "# browser.get(url)\n",
    "# # sleep(3)\n",
    "# source = browser.page_source\n",
    "# browser.quit()\n",
    "\n",
    "# soup = BeautifulSoup(source, 'lxml')\n",
    "# soup.find_all('li', class_='pager-next')[0].text\n",
    "\n",
    "# for link in soup.find_all('a', class_='block-link'):\n",
    "#     print(link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for OpenAI gpt-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('news_articles_business.csv')\n",
    "\n",
    "# index1 = df[df.apply(lambda x: x.astype(str).str.lower())['articles'].str.contains('singapore')].index\n",
    "# index2 = df[df.apply(lambda x: x.astype(str).str.lower())['titles'].str.contains('singapore')].index\n",
    "# index = set(list(index1) + list(index2))\n",
    "# df = df.loc[index, :]\n",
    "\n",
    "# df.to_csv('news_articles_business_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('main_data_old.csv')\n",
    "\n",
    "# first_sentence_list = []\n",
    "# for index, titles in enumerate(df['titles']):\n",
    "#     first_sentence = ''\n",
    "#     first_sentence += titles + '. ' + ' '.join(df.loc[index, 'articles'].split()[:20])\n",
    "#     first_sentence_list.append(first_sentence)\n",
    "# df['first_sentence'] = first_sentence_list\n",
    "\n",
    "# df.to_csv('train_fake_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('main_data_old.csv')\n",
    "\n",
    "# first_sentence_list = []\n",
    "# for index, titles in enumerate(df['titles']):\n",
    "#     first_sentence = ''\n",
    "#     first_sentence += titles + '. ' + df.loc[index, 'articles'] + ' '.join(df.loc[index, 'articles'].split()[:10])\n",
    "#     first_sentence_list.append(first_sentence)\n",
    "# df['first_sentence'] = first_sentence_list\n",
    "\n",
    "# df.to_csv('train_fake_news_full_inputs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape from TODAYonline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scrape for links on main page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.todayonline.com/singapore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in soup.find_all('div', class_ = 'article-listing_f featured-semi'):\n",
    "#     print(i.find('a')['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "# import time\n",
    "\n",
    "# url = \"https://www.todayonline.com/singapore\"\n",
    "# browser = webdriver.Firefox()\n",
    "# browser.get(url)\n",
    "# sleep(10)\n",
    "# page_num = 0\n",
    "\n",
    "# while True:\n",
    "#     try:\n",
    "#         sleep(5)\n",
    "#         elm = browser.find_element_by_link_text('Load more')\n",
    "#         page_num += 1\n",
    "#         print(\"getting page number \"+str(page_num))\n",
    "#         elm.click()\n",
    "        \n",
    "#         if page_num%2 == 0:\n",
    "#             html = browser.page_source.encode('utf-8')\n",
    "#             soup = BeautifulSoup(html, 'lxml')\n",
    "#             print('saving...')\n",
    "#             urls = []\n",
    "#             for i in soup.find_all('div', class_ = 'article-listing_f featured-semi'):\n",
    "#                 urls.append(i.find('a')['href'])\n",
    "#             pd.DataFrame(urls).to_csv('TodayOnlineNews.csv')\n",
    "            \n",
    "#     except:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scrape for articles from the links**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = pd.read_csv('TodayOnlineNewsLinks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls.rename(columns={'0': 'urls'}, inplace = True)\n",
    "urls.drop(columns=['Unnamed: 0'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls['urls'] = 'https://www.todayonline.com' + urls['urls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.todayonline.com/singapore/todays-morning-brief-friday-nov-30'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls['urls'][1594]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://www.todayonline.com/singapore/singapore-work-new-zealand-tackle-terrorism-and-violent-extremism-pm-lee\"\n",
    "\n",
    "# # source = requests.get(url).text\n",
    "# # soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "# browser = webdriver.Firefox()\n",
    "# browser.get(url)\n",
    "# # sleep(3)\n",
    "# source = browser.page_source\n",
    "# browser.quit()\n",
    "# soup = BeautifulSoup(source, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame()\n",
    "\n",
    "# def scrap(feature, source):\n",
    "#             try: \n",
    "#                 feature.append(source)    \n",
    "#             except:\n",
    "#                 feature.append(np.nan)\n",
    "\n",
    "# for index, url in enumerate(urls['urls'][1594:]):\n",
    "    \n",
    "#     loop = True\n",
    "\n",
    "#     while loop == True:\n",
    "#         print('scraping', index, 'out of', len(urls['urls'][1594:]))\n",
    "#         browser = webdriver.Firefox()\n",
    "#         browser.get(url)\n",
    "#         # sleep(3)\n",
    "#         source = browser.page_source\n",
    "#         browser.quit()\n",
    "#         soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "#         titles = []\n",
    "#         authors = []\n",
    "#         links = []\n",
    "#         dates = []\n",
    "#         articles = []\n",
    "        \n",
    "#         try: \n",
    "#             scrap(authors, soup.find('span', class_='today-author').text)\n",
    "#         except:\n",
    "#             authors.append(np.nan)\n",
    "            \n",
    "#         try:\n",
    "#             scrap(titles, soup.find('h1', class_='article-detail_title').text)\n",
    "#         except:\n",
    "#             titles.append(np.nan)\n",
    "        \n",
    "#         try:\n",
    "#             scrap(dates, soup.find('div', class_='article-detail_bylinepublish').text.split('Published')[1].replace(',', ''))\n",
    "#         except:\n",
    "#             dates.append(np.nan)\n",
    "        \n",
    "#         try:\n",
    "#             scrap(articles, soup.find('div', class_='article-detail_body').text)\n",
    "#         except:\n",
    "#             articles.append(np.nan)\n",
    "        \n",
    "#         links.append(url)\n",
    "        \n",
    "#         temp_df = pd.DataFrame([titles, authors, links, dates, articles]).T\n",
    "#         temp_df.columns = [\"titles\", \"authors\", \"urls\", \"dates\", \"articles\"]\n",
    "#         df = pd.concat([df, temp_df])\n",
    "#         df.to_csv('TodayOnlineNews_from1594.csv')\n",
    "\n",
    "#         if articles == []:\n",
    "#             loop = True\n",
    "#         else:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for OpenAI gpt-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('TodayOnlineNewsArticles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3675, 6)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[-df['articles'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3490, 6)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0       0\n",
       "titles           0\n",
       "authors       1082\n",
       "urls             0\n",
       "dates            8\n",
       "articles         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence_list = []\n",
    "for article in df['articles']:\n",
    "    first_sentence = ''\n",
    "    first_sentence += ' '.join(article.split()[:50])\n",
    "    first_sentence_list.append(first_sentence)\n",
    "df['first_sentence'] = first_sentence_list\n",
    "\n",
    "df.to_csv('train_fake_news_today.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
